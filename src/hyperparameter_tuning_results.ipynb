{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = {'mse':0, 'auc':1, 'll':2, 'acc':3, 'perp':4, 'npmi':5, 'shuffle':6, 'shuffle_std':7}\n",
    "metrics = {'mse':0, 'auc':1, 'll':2, 'acc':3, 'perp':4, 'npmi':5, 'shuffle':6}\n",
    "classification_settings = {'peerread', 'yelp', 'amazon_binary', 'framing_corpus'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_results_regression(exp_results, data, model):\n",
    "    best_npmi=-1e7\n",
    "    best_npmi_config=None\n",
    "    best_mse=1e7\n",
    "    best_mse_config=None\n",
    "    best_perp = 1e8\n",
    "    best_perp_config=None\n",
    "    \n",
    "\n",
    "    for config, res in exp_results[data][model].items():\n",
    "        mean_results = res.mean(axis=0)\n",
    "        mse=mean_results[metrics['mse']]\n",
    "        perp=mean_results[metrics['perp']]\n",
    "        npmi=mean_results[metrics['npmi']]\n",
    "        if mse < best_mse:\n",
    "            best_mse=mse\n",
    "            best_mse_config=config\n",
    "        if perp < best_perp:\n",
    "            best_perp=perp\n",
    "            best_perp_config=config\n",
    "        if npmi > best_npmi:\n",
    "            best_npmi=npmi\n",
    "            best_npmi_config=config\n",
    "    \n",
    "    return best_npmi_config, best_mse_config, best_perp_config\n",
    "\n",
    "def get_best_results_classification(exp_results, data, model):\n",
    "    best_npmi=-1e7\n",
    "    best_npmi_config=None\n",
    "    best_acc=0.\n",
    "    best_acc_config=None\n",
    "    best_perp = 1e8\n",
    "    best_perp_config=None\n",
    "\n",
    "    for config, res in exp_results[data][model].items():\n",
    "        mean_results = res.mean(axis=0)\n",
    "        acc=mean_results[metrics['acc']]\n",
    "        perp=mean_results[metrics['perp']]\n",
    "        npmi=mean_results[metrics['npmi']]\n",
    "        if acc > best_acc:\n",
    "            best_acc=acc\n",
    "            best_acc_config=config\n",
    "        if perp < best_perp:\n",
    "            best_perp=perp\n",
    "            best_perp_config=config\n",
    "        if npmi > best_npmi:\n",
    "            best_npmi=npmi\n",
    "            best_npmi_config=config\n",
    "    \n",
    "    return best_npmi_config, best_acc_config, best_perp_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['hstm', 'hstm-nobeta', 'hstm-all']\n",
    "stm_models = ['stm', 'stm+bow']\n",
    "\n",
    "datasets = ['amazon', 'amazon_binary', 'peerread', 'yelp']\n",
    "\n",
    "out = '../out/'\n",
    "\n",
    "Cs = [1e-4, 5e-5, 1e-5, 5e-6, 1e-6]\n",
    "Ctopics = [1e-4, 5e-5, 1e-5, 5e-6, 1e-6]\n",
    "num_folds = 5\n",
    "n_metrics = len(metrics.keys())\n",
    "exp_results = {data:{model:{(C,Ct):np.zeros((num_folds,n_metrics)) for C in Cs for Ct in Ctopics}\n",
    "                     for model in models}for data in datasets}\n",
    "for data in datasets:\n",
    "    for sm in stm_models:\n",
    "        exp_results[data].update({sm:{(C,1e-6):np.zeros((num_folds,n_metrics)) for C in Cs}})\n",
    "\n",
    "for data in datasets:\n",
    "    datadir = data + '.lda_pretrained'\n",
    "    for model in stm_models + models:\n",
    "        for C in Cs:\n",
    "            if model in stm_models:\n",
    "                for e_idx in range(num_folds):\n",
    "                    resfile= out + datadir + '/' + model + '.result.split'+str(e_idx) + '.setting=' + str((C,1e-6))\n",
    "                    results = np.load(resfile + '.npy')\n",
    "                    exp_results[data][model][(C,1e-6)][e_idx]=results\n",
    "            else:\n",
    "                for Ct in Ctopics:\n",
    "                    for e_idx in range(num_folds):\n",
    "                        resfile= out + datadir + '/' + model + '.result.split'+str(e_idx) + '.setting=' + str((C,Ct))\n",
    "                        results = np.load(resfile + '.npy')\n",
    "                        exp_results[data][model][(C,Ct)][e_idx]=results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Dataset: amazon\n",
      "**********\n",
      "Model: hstm (5e-06, 1e-05)\n",
      "MSE: (0.698, 0.035) NPMI: 15.939 Perplexity: 297.62 Shuffle loss: 0.0\n",
      "**********\n",
      "Model: hstm-nobeta (0.0001, 1e-06)\n",
      "MSE: (0.704, 0.035) NPMI: 15.965 Perplexity: 312.22 Shuffle loss: 0.0\n",
      "**********\n",
      "Model: hstm-all (5e-06, 1e-05)\n",
      "MSE: (0.702, 0.034) NPMI: 15.843 Perplexity: 282.926 Shuffle loss: 0.0\n",
      "**********\n",
      "Model: stm (5e-05, 1e-06)\n",
      "MSE: (0.931, 0.046) NPMI: 15.847 Perplexity: 313.459 Shuffle loss: 0.0\n",
      "**********\n",
      "Model: stm+bow (5e-06, 1e-06)\n",
      "MSE: (0.716, 0.035) NPMI: 15.935 Perplexity: 357.34 Shuffle loss: 0.0\n",
      "********************\n",
      "Dataset: amazon_binary\n",
      "**********\n",
      "Model: hstm (5e-06, 1e-06)\n",
      "Accuracy: (0.927, 0.004) AUC: (0.926, 0.005) Log loss: (0.187, 0.003) NPMI: 9.886 Perplexity: 30.464 Shuffle loss: 0.0\n",
      "**********\n",
      "Model: hstm-nobeta (1e-06, 1e-06)\n",
      "Accuracy: (0.91, 0.003) AUC: (0.919, 0.004) Log loss: (0.215, 0.004) NPMI: 9.65 Perplexity: 31.372 Shuffle loss: 0.0\n",
      "**********\n",
      "Model: hstm-all (5e-06, 1e-06)\n",
      "Accuracy: (0.928, 0.005) AUC: (0.925, 0.003) Log loss: (0.186, 0.003) NPMI: 9.396 Perplexity: 32.808 Shuffle loss: 0.0\n",
      "**********\n",
      "Model: stm (1e-06, 1e-06)\n",
      "Accuracy: (0.895, 0.003) AUC: (0.75, 0.013) Log loss: (0.311, 0.008) NPMI: 9.594 Perplexity: 33.98 Shuffle loss: 0.0\n",
      "**********\n",
      "Model: stm+bow (1e-06, 1e-06)\n",
      "Accuracy: (0.908, 0.005) AUC: (0.917, 0.003) Log loss: (0.219, 0.005) NPMI: 9.898 Perplexity: 29.579 Shuffle loss: 0.0\n",
      "********************\n",
      "Dataset: peerread\n",
      "**********\n",
      "Model: hstm (1e-05, 1e-06)\n",
      "Accuracy: (0.81, 0.009) AUC: (0.847, 0.018) Log loss: (0.402, 0.011) NPMI: 7.871 Perplexity: 1063.114 Shuffle loss: 0.0\n",
      "**********\n",
      "Model: hstm-nobeta (5e-06, 5e-05)\n",
      "Accuracy: (0.802, 0.009) AUC: (0.844, 0.017) Log loss: (0.413, 0.008) NPMI: 7.86 Perplexity: 1168.66 Shuffle loss: 0.0\n",
      "**********\n",
      "Model: hstm-all (1e-05, 1e-06)\n",
      "Accuracy: (0.81, 0.01) AUC: (0.85, 0.018) Log loss: (0.398, 0.012) NPMI: 8.009 Perplexity: 1080.016 Shuffle loss: 0.0\n",
      "**********\n",
      "Model: stm (0.0001, 1e-06)\n",
      "Accuracy: (0.777, 0.006) AUC: (0.802, 0.017) Log loss: (0.457, 0.005) NPMI: 7.788 Perplexity: 1340.834 Shuffle loss: 0.0\n",
      "**********\n",
      "Model: stm+bow (5e-06, 1e-06)\n",
      "Accuracy: (0.8, 0.01) AUC: (0.843, 0.017) Log loss: (0.413, 0.008) NPMI: 7.794 Perplexity: 1014.076 Shuffle loss: 0.0\n",
      "********************\n",
      "Dataset: yelp\n",
      "**********\n",
      "Model: hstm (1e-06, 1e-06)\n",
      "Accuracy: (0.912, 0.008) AUC: (0.967, 0.002) Log loss: (0.245, 0.006) NPMI: 10.352 Perplexity: 82.893 Shuffle loss: 0.0\n",
      "**********\n",
      "Model: hstm-nobeta (1e-05, 1e-06)\n",
      "Accuracy: (0.896, 0.01) AUC: (0.96, 0.004) Log loss: (0.295, 0.007) NPMI: 10.479 Perplexity: 87.144 Shuffle loss: 0.0\n",
      "**********\n",
      "Model: hstm-all (5e-05, 1e-06)\n",
      "Accuracy: (0.913, 0.005) AUC: (0.968, 0.003) Log loss: (0.239, 0.006) NPMI: 10.522 Perplexity: 92.167 Shuffle loss: 0.0\n",
      "**********\n",
      "Model: stm (1e-05, 1e-06)\n",
      "Accuracy: (0.833, 0.005) AUC: (0.904, 0.005) Log loss: (0.451, 0.005) NPMI: 10.266 Perplexity: 87.413 Shuffle loss: 0.0\n",
      "**********\n",
      "Model: stm+bow (1e-05, 1e-06)\n",
      "Accuracy: (0.893, 0.008) AUC: (0.959, 0.004) Log loss: (0.299, 0.006) NPMI: 10.479 Perplexity: 89.483 Shuffle loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "for data in datasets:\n",
    "    print(\"*\"*20)\n",
    "    print(\"Dataset:\", data)\n",
    "    for model in models + stm_models:\n",
    "        if data not in classification_settings:\n",
    "            c_nmpi, c_mse, c_perp = get_best_results_regression(exp_results, data, model)\n",
    "            results = exp_results[data][model][c_mse].mean(axis=0)\n",
    "            stds = exp_results[data][model][c_mse].std(axis=0)\n",
    "            results = np.round(results, 3)\n",
    "            stds = np.round(stds, 3)\n",
    "            \n",
    "            print(\"*\"*10)\n",
    "            print(\"Model:\", model, c_mse)\n",
    "            print(\"MSE:\", (results[metrics['mse']],\n",
    "                  stds[metrics['mse']]), \n",
    "                  \"NPMI:\", results[metrics['npmi']], \n",
    "                  'Perplexity:', results[metrics['perp']],\n",
    "                    'Shuffle loss:', results[metrics['shuffle']])\n",
    "        else:\n",
    "            c_nmpi, c_mse, c_perp = get_best_results_classification(exp_results, data, model)\n",
    "            results = exp_results[data][model][c_mse].mean(axis=0)\n",
    "            stds = exp_results[data][model][c_mse].std(axis=0)\n",
    "            results = np.round(results, 3)\n",
    "            stds = np.round(stds, 3)\n",
    "            print(\"*\"*10)\n",
    "            print(\"Model:\", model, c_mse)\n",
    "            print(\"Accuracy:\", (results[metrics['acc']], stds[metrics['acc']]),\n",
    "                  \"AUC:\", (results[metrics['auc']], stds[metrics['auc']]),\n",
    "                  \"Log loss:\", (results[metrics['ll']], stds[metrics['ll']]),\n",
    "                  \"NPMI:\", results[metrics['npmi']], \n",
    "                  'Perplexity:', results[metrics['perp']],\n",
    "                 'Shuffle loss:', results[metrics['shuffle']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['hstm-all']\n",
    "stm_models = ['stm', 'stm+bow']\n",
    "data = 'framing_corpus'\n",
    "topics = ['deathpenalty', 'immigration', 'guncontrol', 'samesex']\n",
    "\n",
    "out = '../out/'\n",
    "\n",
    "Cs = [1e-4, 5e-5, 1e-5, 5e-6, 1e-6]\n",
    "Ctopics = [1e-4, 5e-5, 1e-5, 5e-6, 1e-6]\n",
    "num_folds = 5\n",
    "n_metrics = len(metrics.keys())\n",
    "exp_results = {topic:{model:{(C,Ct):np.zeros((num_folds,n_metrics)) for C in Cs for Ct in Ctopics}\n",
    "                     for model in models} for topic in topics}\n",
    "for topic in topics:\n",
    "    for sm in stm_models:\n",
    "        exp_results[topic].update({sm:{(C,1e-6):np.zeros((num_folds,n_metrics)) for C in Cs}})\n",
    "\n",
    "for topic in topics:\n",
    "    datadir = data + '_' + topic + '.lda_pretrained'\n",
    "    for model in stm_models + models:\n",
    "        for C in Cs:\n",
    "            if model in stm_models:\n",
    "                for e_idx in range(num_folds):\n",
    "                    resfile= out + datadir + '/' + model + '.result.split'+str(e_idx) + '.setting=' + str((C,1e-6))\n",
    "                    results = np.load(resfile + '.npy')\n",
    "                    exp_results[topic][model][(C,1e-6)][e_idx]=results\n",
    "            else:\n",
    "                for Ct in Ctopics:\n",
    "                    for e_idx in range(num_folds):\n",
    "                        resfile= out + datadir + '/' + model + '.result.split'+str(e_idx) + '.setting=' + str((C,Ct))\n",
    "                        results = np.load(resfile + '.npy')\n",
    "                        exp_results[topic][model][(C,Ct)][e_idx]=results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Topic: deathpenalty\n",
      "**********\n",
      "Model: hstm-all (5e-06, 5e-06)\n",
      "Accuracy: (0.785, 0.023) AUC: (0.859, 0.01) Log loss: (0.469, 0.013) NPMI: 4.041 Perplexity: 322.145 Shuffle loss: 0.0\n",
      "**********\n",
      "Model: stm (5e-05, 1e-06)\n",
      "Accuracy: (0.689, 0.014) AUC: (0.747, 0.011) Log loss: (0.596, 0.009) NPMI: 4.243 Perplexity: 358.056 Shuffle loss: 0.0\n",
      "**********\n",
      "Model: stm+bow (5e-05, 1e-06)\n",
      "Accuracy: (0.759, 0.014) AUC: (0.824, 0.009) Log loss: (0.527, 0.008) NPMI: 4.262 Perplexity: 314.19 Shuffle loss: 0.0\n",
      "********************\n",
      "Topic: immigration\n",
      "**********\n",
      "Model: hstm-all (1e-05, 1e-06)\n",
      "Accuracy: (0.804, 0.014) AUC: (0.866, 0.009) Log loss: (0.439, 0.014) NPMI: 4.105 Perplexity: 472.945 Shuffle loss: 0.0\n",
      "**********\n",
      "Model: stm (1e-05, 1e-06)\n",
      "Accuracy: (0.721, 0.016) AUC: (0.775, 0.011) Log loss: (0.547, 0.01) NPMI: 4.858 Perplexity: 636.971 Shuffle loss: 0.0\n",
      "**********\n",
      "Model: stm+bow (1e-06, 1e-06)\n",
      "Accuracy: (0.775, 0.023) AUC: (0.839, 0.008) Log loss: (0.488, 0.011) NPMI: 4.047 Perplexity: 630.302 Shuffle loss: 0.0\n",
      "********************\n",
      "Topic: guncontrol\n",
      "**********\n",
      "Model: hstm-all (5e-05, 1e-06)\n",
      "Accuracy: (0.779, 0.015) AUC: (0.818, 0.015) Log loss: (0.471, 0.015) NPMI: 7.185 Perplexity: 542.104 Shuffle loss: 0.0\n",
      "**********\n",
      "Model: stm (0.0001, 1e-06)\n",
      "Accuracy: (0.694, 0.016) AUC: (0.671, 0.019) Log loss: (0.582, 0.012) NPMI: 7.957 Perplexity: 696.506 Shuffle loss: 0.0\n",
      "**********\n",
      "Model: stm+bow (1e-06, 1e-06)\n",
      "Accuracy: (0.728, 0.007) AUC: (0.776, 0.015) Log loss: (0.525, 0.004) NPMI: 7.591 Perplexity: 707.686 Shuffle loss: 0.0\n",
      "********************\n",
      "Topic: samesex\n",
      "**********\n",
      "Model: hstm-all (5e-05, 1e-06)\n",
      "Accuracy: (0.826, 0.009) AUC: (0.875, 0.016) Log loss: (0.384, 0.025) NPMI: 7.002 Perplexity: 1494.594 Shuffle loss: 0.0\n",
      "**********\n",
      "Model: stm (0.0001, 1e-06)\n",
      "Accuracy: (0.769, 0.013) AUC: (0.751, 0.014) Log loss: (0.501, 0.014) NPMI: 7.432 Perplexity: inf Shuffle loss: 0.0\n",
      "**********\n",
      "Model: stm+bow (1e-06, 1e-06)\n",
      "Accuracy: (0.805, 0.009) AUC: (0.845, 0.016) Log loss: (0.426, 0.015) NPMI: 7.034 Perplexity: 1445.717 Shuffle loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhanyasridhar/opt/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:193: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n"
     ]
    }
   ],
   "source": [
    "for topic in topics:\n",
    "    print(\"*\"*20)\n",
    "    print(\"Topic:\", topic)\n",
    "    for model in models + stm_models:\n",
    "        if data not in classification_settings:\n",
    "            c_nmpi, c_mse, c_perp = get_best_results_regression(exp_results, topic, model)\n",
    "            results = exp_results[topic][model][c_mse].mean(axis=0)\n",
    "            stds = exp_results[topic][model][c_mse].std(axis=0)\n",
    "            results = np.round(results, 3)\n",
    "            stds = np.round(stds, 3)\n",
    "            \n",
    "            print(\"*\"*10)\n",
    "            print(\"Model:\", model, c_mse)\n",
    "            print(\"MSE:\", (results[metrics['mse']],\n",
    "                  stds[metrics['mse']]), \n",
    "                  \"NPMI:\", results[metrics['npmi']], \n",
    "                  'Perplexity:', results[metrics['perp']],\n",
    "                    'Shuffle loss:', results[metrics['shuffle']])\n",
    "        else:\n",
    "            c_nmpi, c_mse, c_perp = get_best_results_classification(exp_results, topic, model)\n",
    "            results = exp_results[topic][model][c_mse].mean(axis=0)\n",
    "            stds = exp_results[topic][model][c_mse].std(axis=0)\n",
    "            results = np.round(results, 3)\n",
    "            stds = np.round(stds, 3)\n",
    "            print(\"*\"*10)\n",
    "            print(\"Model:\", model, c_mse)\n",
    "            print(\"Accuracy:\", (results[metrics['acc']], stds[metrics['acc']]),\n",
    "                  \"AUC:\", (results[metrics['auc']], stds[metrics['auc']]),\n",
    "                  \"Log loss:\", (results[metrics['ll']], stds[metrics['ll']]),\n",
    "                  \"NPMI:\", results[metrics['npmi']], \n",
    "                  'Perplexity:', results[metrics['perp']],\n",
    "                 'Shuffle loss:', results[metrics['shuffle']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
